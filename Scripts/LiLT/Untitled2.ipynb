{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817f703-7b9c-405e-b058-59b15414a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9966017-7161-4558-be70-34bd8912501b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import LiltForTokenClassification, LayoutLMv3Processor\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    " \n",
    "# load model and processor from huggingface hub\n",
    "model = LiltForTokenClassification.from_pretrained(\"philschmid/lilt-en-funsd\")\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"philschmid/lilt-en-funsd\")\n",
    " \n",
    " \n",
    "# helper function to unnormalize bboxes for drawing onto the image\n",
    "def unnormalize_box(bbox, width, height):\n",
    "    return [\n",
    "        width * (bbox[0] / 1000),\n",
    "        height * (bbox[1] / 1000),\n",
    "        width * (bbox[2] / 1000),\n",
    "        height * (bbox[3] / 1000),\n",
    "    ]\n",
    " \n",
    " \n",
    "label2color = {\n",
    "    \"B-HEADER\": \"blue\",\n",
    "    \"B-QUESTION\": \"red\",\n",
    "    \"B-ANSWER\": \"green\",\n",
    "    \"I-HEADER\": \"blue\",\n",
    "    \"I-QUESTION\": \"red\",\n",
    "    \"I-ANSWER\": \"green\",\n",
    "}\n",
    "# draw results onto the image\n",
    "def draw_boxes(image, boxes, predictions):\n",
    "    width, height = image.size\n",
    "    normalizes_boxes = [unnormalize_box(box, width, height) for box in boxes]\n",
    " \n",
    "    # draw predictions over the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "    for prediction, box in zip(predictions, normalizes_boxes):\n",
    "        if prediction == \"O\":\n",
    "            continue\n",
    "        draw.rectangle(box, outline=\"black\")\n",
    "        draw.rectangle(box, outline=label2color[prediction])\n",
    "        draw.text((box[0] + 10, box[1] - 10), text=prediction, fill=label2color[prediction], font=font)\n",
    "    return image\n",
    " \n",
    " \n",
    "# run inference\n",
    "def run_inference(image, model=model, processor=processor, output_image=True):\n",
    "    # create model input\n",
    "    encoding = processor(image, return_tensors=\"pt\")\n",
    "    del encoding[\"pixel_values\"]\n",
    "    # run inference\n",
    "    outputs = model(**encoding)\n",
    "    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "    # get labels\n",
    "    labels = [model.config.id2label[prediction] for prediction in predictions]\n",
    "    if output_image:\n",
    "        return draw_boxes(image, encoding[\"bbox\"][0], labels)\n",
    "    else:\n",
    "        return labels\n",
    " \n",
    " \n",
    "# run_inference(dataset[\"test\"][34][\"image\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de3590-768d-46ec-9783-e0ab69ee7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Load image from a file\n",
    "image_path = \"./pages/image.jpg\"  # Update this to your image's file path\n",
    "jpg = Image.open(image_path)\n",
    "jpg\n",
    "image = jpg.convert(\"RGB\")\n",
    "\n",
    "print(\"## Input\")      \n",
    "image\n",
    "\n",
    "# Run inference\n",
    "output_image = run_inference(image)\n",
    "\n",
    "print(\"## Output\")      \n",
    "# Display the annotated image in Jupyter\n",
    "output_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e0c53c1-02b5-4f06-8234-3640c380980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_with_details(image, model=model, processor=processor):\n",
    "    # Ensure proper image preprocessing (resize, normalize, etc.)\n",
    "    encoding = processor(image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Check the sizes of input_ids and bbox\n",
    "    if \"input_ids\" not in encoding or \"bbox\" not in encoding:\n",
    "        raise ValueError(\"Missing required encoding fields: 'input_ids' or 'bbox'\")\n",
    "    \n",
    "    # Run inference on the image\n",
    "    try:\n",
    "        outputs = model(**encoding)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model inference: {e}\")\n",
    "        raise\n",
    "    \n",
    "    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "    \n",
    "    # Ensure that we only work with valid tokens\n",
    "    labels = [model.config.id2label[pred] for pred in predictions]\n",
    "    bboxes = encoding.get(\"bbox\", [])[0]  # Safely get the bounding boxes\n",
    "    words = encoding.get(\"input_ids\", [])[0]  # Safely get the input_ids\n",
    "    word_texts = processor.tokenizer.decode(words, skip_special_tokens=True).split()\n",
    "\n",
    "    # Collect detailed results\n",
    "    results = []\n",
    "    for label, bbox, word in zip(labels, bboxes, word_texts):\n",
    "        if label != \"O\":  # Skip non-entity tokens\n",
    "            results.append({\n",
    "                \"ocr_value\": word,\n",
    "                \"position\": {\n",
    "                    \"x_min\": bbox[0],\n",
    "                    \"y_min\": bbox[1],\n",
    "                    \"x_max\": bbox[2],\n",
    "                    \"y_max\": bbox[3],\n",
    "                },\n",
    "                \"data_type\": label\n",
    "            })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d043a97a-3c29-41f5-a6e3-b8415b3eaa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: image.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure the output directory exists\n",
    "input_dir = \"./pages\"\n",
    "output_dir = \"./annotated\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List all PNG files in the input directory\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith((\".png\", \".jpeg\", \".jpg\"))]\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    output_image_path = os.path.join(output_dir, f\"annotated_{file}\")\n",
    "    output_data_path = os.path.join(output_dir, f\"{file.split('.')[0]}_data.json\")\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    \n",
    "    # Run inference\n",
    "    labels = run_inference(image, output_image=False)\n",
    "    annotated_image = run_inference(image, output_image=True)\n",
    "    \n",
    "    # Save the annotated image\n",
    "    annotated_image.save(output_image_path)\n",
    "    \n",
    "    # Save the returned labels (data) as JSON\n",
    "    with open(output_data_path, \"w\") as f:\n",
    "        json.dump(labels, f, indent=4)\n",
    "    \n",
    "    print(f\"Processed and saved: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048131ff-ffc7-4ecb-9058-36128e9d85b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: image.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure the output directory exists\n",
    "input_dir = \"./pages\"\n",
    "output_dir = \"./annotated\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to match questions and answers based on position\n",
    "def match_question_answer(data):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    # Separate questions and answers based on their type\n",
    "    for entry in data:\n",
    "        if entry[\"data_type\"] == \"B-QUESTION\" or entry[\"data_type\"] == \"I-QUESTION\":\n",
    "            questions.append(entry)\n",
    "        elif entry[\"data_type\"] == \"B-ANSWER\" or entry[\"data_type\"] == \"I-ANSWER\":\n",
    "            answers.append(entry)\n",
    "    \n",
    "    # Attempt to match questions with answers based on vertical position (y_min)\n",
    "    paired_data = []\n",
    "    for question in questions:\n",
    "        closest_answer = None\n",
    "        for answer in answers:\n",
    "            # Match question and answer by finding the closest answer vertically (y_min of answer should be after y_max of question)\n",
    "            if answer[\"position\"][\"y_min\"] > question[\"position\"][\"y_max\"]:\n",
    "                if closest_answer is None or answer[\"position\"][\"y_min\"] < closest_answer[\"position\"][\"y_min\"]:\n",
    "                    closest_answer = answer\n",
    "        if closest_answer:\n",
    "            paired_data.append({\n",
    "                \"question\": question[\"ocr_value\"],\n",
    "                \"answer\": closest_answer[\"ocr_value\"],\n",
    "                \"question_position\": question[\"position\"],\n",
    "                \"answer_position\": closest_answer[\"position\"]\n",
    "            })\n",
    "            answers.remove(closest_answer)  # Remove the matched answer to avoid reuse\n",
    "    \n",
    "    return paired_data\n",
    "\n",
    "# Updated run_inference function to return detailed data\n",
    "def run_inference_with_details(image, model=model, processor=processor):\n",
    "    # Prepare model input\n",
    "    encoding = processor(image, return_tensors=\"pt\")\n",
    "    del encoding[\"pixel_values\"]\n",
    "    # Run inference\n",
    "    outputs = model(**encoding)\n",
    "    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "    # Get labels\n",
    "    labels = [model.config.id2label[pred] for pred in predictions]\n",
    "    bboxes = encoding[\"bbox\"][0].tolist()\n",
    "    words = encoding[\"input_ids\"][0].tolist()\n",
    "    word_texts = processor.tokenizer.decode(words).split()\n",
    "\n",
    "    # Collect detailed results\n",
    "    results = []\n",
    "    for label, bbox, word in zip(labels, bboxes, word_texts):\n",
    "        if label != \"O\":  # Skip non-entity tokens\n",
    "            results.append({\n",
    "                \"ocr_value\": word,\n",
    "                \"position\": {\n",
    "                    \"x_min\": bbox[0],\n",
    "                    \"y_min\": bbox[1],\n",
    "                    \"x_max\": bbox[2],\n",
    "                    \"y_max\": bbox[3],\n",
    "                },\n",
    "                \"data_type\": label\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# List all JPEG and PNG files in the input directory\n",
    "files = [f for f in os.listdir(input_dir) if f.lower().endswith((\".png\", \".jpeg\", \".jpg\"))]\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    output_image_path = os.path.join(output_dir, f\"annotated_{file}\")\n",
    "    output_data_path = os.path.join(output_dir, f\"{file.split('.')[0]}_data.json\")\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    \n",
    "    # Run inference\n",
    "    detailed_data = run_inference_with_details(image)\n",
    "    paired_data = match_question_answer(detailed_data)  # Match questions and answers\n",
    "    annotated_image = run_inference(image, output_image=True)\n",
    "    \n",
    "    # Save the annotated image\n",
    "    annotated_image.save(output_image_path)\n",
    "    \n",
    "    # Save the detailed question-answer matched data as JSON\n",
    "    with open(output_data_path, \"w\") as f:\n",
    "        json.dump(paired_data, f, indent=4)\n",
    "    \n",
    "    print(f\"Processed and saved: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "836a5678-1ca3-41e3-8283-49246a6a958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote annotated/detailed_data_image.jpg.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Example loop to process all images\n",
    "image_folder = \"pages\"\n",
    "output_folder = \"annotated\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    file_path = os.path.join(image_folder, filename)\n",
    "    \n",
    "    if file_path.endswith(('.png', '.jpg', '.jpeg')):  # Process only image files\n",
    "        try:\n",
    "            image = Image.open(file_path).convert(\"RGB\")\n",
    "            \n",
    "            # Run inference and get detailed data\n",
    "            detailed_data = run_inference_with_details(image)\n",
    "            \n",
    "            if detailed_data is not None:               \n",
    "\n",
    "                detailed_data_fileName = os.path.join(output_folder, f\"detailed_data_{filename}.json\")\n",
    "                with open(detailed_data_fileName, \"w\") as f:\n",
    "                    json.dump(detailed_data, f, indent=4)\n",
    "                    print(f\"wrote {detailed_data_fileName}\")\n",
    "                    \n",
    "                # Match question and answer\n",
    "                paired_data = match_question_answer(detailed_data) \n",
    "                \n",
    "                # Annotate the image and save the result\n",
    "                annotated_image = run_inference(image, output_image=True)\n",
    "                annotated_image.save(os.path.join(output_folder, f\"annotated_{filename}\"))\n",
    "                \n",
    "                # Optionally save the paired data\n",
    "                with open(os.path.join(output_folder, f\"paired_{filename}.json\"), 'w') as f:\n",
    "                    json.dump(paired_data, f, indent=4)\n",
    "            else:\n",
    "                print(f\"Skipping file {filename} due to error.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "            continue  # Skip the file and move to the next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d1710-4321-4957-848e-a834940e87ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
